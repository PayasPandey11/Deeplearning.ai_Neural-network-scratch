{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN from scratch to generate new text given a starting character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1607894 78\n"
     ]
    }
   ],
   "source": [
    "data = open('/home/payas/dl_box/data/GOT_books/001ssb.txt','r').read()\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size =  len(data), len(chars)\n",
    "\n",
    "print(data_size,vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Y': 0, '\\n': 1, 'g': 2, 'c': 3, '\"': 4, '-': 5, 'G': 6, ',': 7, '2': 8, 'z': 9, 'C': 10, '4': 11, 'K': 12, '6': 13, '`': 14, 'l': 15, '3': 16, '.': 17, 'I': 18, 'D': 19, 'i': 20, 's': 21, 'H': 22, '!': 23, 'm': 24, 'B': 25, ']': 26, 'b': 27, 'v': 28, 'q': 29, '1': 30, '8': 31, 'u': 32, '*': 33, 'f': 34, 't': 35, ')': 36, 'p': 37, 'x': 38, 'V': 39, 'e': 40, 'k': 41, 'P': 42, 'L': 43, 'w': 44, '?': 45, 'Q': 46, '(': 47, 'y': 48, '~': 49, 'h': 50, 'N': 51, ':': 52, '9': 53, 'S': 54, 'U': 55, 'A': 56, 'T': 57, 'M': 58, 'E': 59, 'F': 60, ' ': 61, \"'\": 62, ';': 63, 'J': 64, '0': 65, '5': 66, 'o': 67, 'a': 68, 'd': 69, 'R': 70, 'n': 71, 'O': 72, '7': 73, 'W': 74, 'X': 75, 'r': 76, 'j': 77} {0: 'Y', 1: '\\n', 2: 'g', 3: 'c', 4: '\"', 5: '-', 6: 'G', 7: ',', 8: '2', 9: 'z', 10: 'C', 11: '4', 12: 'K', 13: '6', 14: '`', 15: 'l', 16: '3', 17: '.', 18: 'I', 19: 'D', 20: 'i', 21: 's', 22: 'H', 23: '!', 24: 'm', 25: 'B', 26: ']', 27: 'b', 28: 'v', 29: 'q', 30: '1', 31: '8', 32: 'u', 33: '*', 34: 'f', 35: 't', 36: ')', 37: 'p', 38: 'x', 39: 'V', 40: 'e', 41: 'k', 42: 'P', 43: 'L', 44: 'w', 45: '?', 46: 'Q', 47: '(', 48: 'y', 49: '~', 50: 'h', 51: 'N', 52: ':', 53: '9', 54: 'S', 55: 'U', 56: 'A', 57: 'T', 58: 'M', 59: 'E', 60: 'F', 61: ' ', 62: \"'\", 63: ';', 64: 'J', 65: '0', 66: '5', 67: 'o', 68: 'a', 69: 'd', 70: 'R', 71: 'n', 72: 'O', 73: '7', 74: 'W', 75: 'X', 76: 'r', 77: 'j'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "print(char_to_ix,ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size,1))\n",
    "vector_for_char_a[char_to_ix['s']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 500\n",
    "seq_length = 25\n",
    "learning_rate = 0.03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 \n",
    "\n",
    "bh = np.zeros ((hidden_size,1))\n",
    "by = np.zeros((vocab_size,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    \n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0 \n",
    "    \n",
    "    #Forward\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1))\n",
    "        xs[t][inputs[t]] = 1 \n",
    "        \n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]), + bh)\n",
    "        ys[t] = np.dot(Why, hs[t]) + by\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "        \n",
    "        loss += -np.log(ps[t][targets[t],0])\n",
    "        \n",
    "    #Backward\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        #output probabilities\n",
    "        dy = np.copy(ps[t])\n",
    "        #derive our first gradient\n",
    "        dy[targets[t]] -= 1 # backprop into y  \n",
    "        #compute output gradient -  output times hidden states transpose\n",
    "        #When we apply the transpose weight matrix,  \n",
    "        #we can think intuitively of this as moving the error backward\n",
    "        #through the network, giving us some sort of measure of the error \n",
    "        #at the output of the lth layer. \n",
    "        #output gradient\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        #derivative of output bias\n",
    "        dby += dy\n",
    "        #backpropagate!\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "        dbh += dhraw #derivative of hidden bias\n",
    "        dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Whh.T, dhraw) \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`PG5k`by23I7(,kGg5.S0Yy5snfead\n",
      "u)CSl!vIzyK(;oX8\n",
      "Hnv28GcE]6DtzSld!uqu\n",
      "Ig4';8O,W:\n",
      "d\n",
      "w..'T8C)JxypYqqlmtK0oifc'yy(,M~T;L~1o6'cXQ4HC8q;(6tXcgp92!5)up0*,SVv4IyQ6pkV',cCSF44qo-BEqako\"(qMA5wS5ruCbSmLx:e-IoNt!\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"                                                                                                                                                                                         \n",
    "    sample a sequence of integers from the model                                                                                                                                                \n",
    "    h is memory state, seed_ix is seed letter for first time step   \n",
    "    n is how many characters to predict\n",
    "    \"\"\"\n",
    "    #create vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for our seed char\n",
    "    x[seed_ix] = 1\n",
    "    #list to store generated chars\n",
    "    ixes = []\n",
    "    #for as many characters as we want to generate\n",
    "    for t in range(n):\n",
    "        #a hidden state at a given time step is a function \n",
    "        #of the input at the same time step modified by a weight matrix \n",
    "        #added to the hidden state of the previous time step \n",
    "        #multiplied by its own hidden state to hidden state matrix.\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        #compute output (unnormalised)\n",
    "        y = np.dot(Why, h) + by\n",
    "        ## probabilities for next chars\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        #pick one with the highest probability \n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        #create a vector\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        #customize it for the predicted char\n",
    "        x[ix] = 1\n",
    "        #add it to the list\n",
    "        ixes.append(ix)\n",
    "\n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print (txt)\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0\n",
      "loss: 108.917709635\n",
      "iter: 1000\n",
      "loss: 99.5320405749\n",
      "iter: 2000\n",
      "loss: 86.0123841408\n",
      "iter: 3000\n",
      "loss: 77.2816506481\n",
      "iter: 4000\n",
      "loss: 71.2628048417\n",
      "iter: 5000\n",
      "loss: 67.5329779409\n",
      "iter: 6000\n",
      "loss: 64.803788743\n",
      "iter: 7000\n",
      "loss: 62.6977612804\n",
      "iter: 8000\n",
      "loss: 61.6331194367\n",
      "iter: 9000\n",
      "loss: 60.7494459572\n",
      "iter: 10000\n",
      "loss: 60.3593812701\n",
      "iter: 11000\n",
      "loss: 60.0039702702\n",
      "iter: 12000\n",
      "loss: 59.2592127204\n",
      "iter: 13000\n",
      "loss: 58.6127187728\n",
      "iter: 14000\n",
      "loss: 58.3662676174\n",
      "iter: 15000\n",
      "loss: 57.5850812703\n",
      "iter: 16000\n",
      "loss: 57.6029838064\n",
      "iter: 17000\n",
      "loss: 56.7388483592\n",
      "iter: 18000\n",
      "loss: 56.381836564\n",
      "iter: 19000\n",
      "loss: 56.2219393161\n",
      "iter: 20000\n",
      "loss: 56.3734279244\n",
      "iter: 21000\n",
      "loss: 56.3098198681\n",
      "iter: 22000\n",
      "loss: 55.9952494477\n",
      "iter: 23000\n",
      "loss: 56.0656037934\n",
      "iter: 24000\n",
      "loss: 55.7120305475\n",
      "iter: 25000\n",
      "loss: 55.5598935273\n",
      "iter: 26000\n",
      "loss: 55.5984610432\n",
      "iter: 27000\n",
      "loss: 55.0856424727\n",
      "iter: 28000\n",
      "loss: 54.7568532381\n",
      "iter: 29000\n",
      "loss: 54.7311960699\n",
      "iter: 30000\n",
      "loss: 54.5738694943\n",
      "iter: 31000\n",
      "loss: 54.8235949392\n",
      "iter: 32000\n",
      "loss: 54.1320285243\n",
      "iter: 33000\n",
      "loss: 53.9934371408\n",
      "iter: 34000\n",
      "loss: 54.229073899\n",
      "iter: 35000\n",
      "loss: 54.1725975848\n",
      "iter: 36000\n",
      "loss: 53.8646490806\n",
      "iter: 37000\n",
      "loss: 53.7273775956\n",
      "iter: 38000\n",
      "loss: 53.7336024977\n",
      "iter: 39000\n",
      "loss: 53.2860760834\n",
      "iter: 40000\n",
      "loss: 52.8939517731\n",
      "iter: 41000\n",
      "loss: 52.9460066638\n",
      "iter: 42000\n",
      "loss: 53.054572355\n",
      "iter: 43000\n",
      "loss: 52.0054764815\n",
      "iter: 44000\n",
      "loss: 52.1531791073\n",
      "iter: 45000\n",
      "loss: 52.2062424247\n",
      "iter: 46000\n",
      "loss: 52.2175243907\n",
      "iter: 47000\n",
      "loss: 52.9711080203\n",
      "iter: 48000\n",
      "loss: 52.6111940369\n",
      "iter: 49000\n",
      "loss: 52.9435275114\n",
      "iter: 50000\n",
      "loss: 52.3077863867\n",
      "iter: 51000\n",
      "loss: 52.462411543\n",
      "iter: 52000\n",
      "loss: 52.3337831657\n",
      "iter: 53000\n",
      "loss: 52.19119347\n",
      "iter: 54000\n",
      "loss: 52.6607350414\n",
      "iter: 55000\n",
      "loss: 52.3348020391\n",
      "iter: 56000\n",
      "loss: 51.9845102574\n",
      "iter: 57000\n",
      "loss: 52.3267172481\n",
      "iter: 58000\n",
      "loss: 51.586720734\n",
      "iter: 59000\n",
      "loss: 51.6050833611\n",
      "iter: 60000\n",
      "loss: 50.7145870771\n",
      "iter: 61000\n",
      "loss: 51.0947145295\n",
      "iter: 62000\n",
      "loss: 51.1823041198\n",
      "iter: 63000\n",
      "loss: 51.0545464314\n",
      "iter: 64000\n",
      "loss: 51.3548480713\n",
      "iter: 65000\n",
      "loss: 51.268931321\n",
      "iter: 66000\n",
      "loss: 50.6395617715\n",
      "iter: 67000\n",
      "loss: 51.1401665807\n",
      "iter: 68000\n",
      "loss: 50.8009814662\n",
      "iter: 69000\n",
      "loss: 50.8061792566\n",
      "iter: 70000\n",
      "loss: 51.0041715218\n",
      "iter: 71000\n",
      "loss: 50.4229695256\n",
      "iter: 72000\n",
      "loss: 50.5936590022\n",
      "iter: 73000\n",
      "loss: 50.7298236301\n",
      "iter: 74000\n",
      "loss: 50.8658121702\n",
      "iter: 75000\n",
      "loss: 50.7785243187\n",
      "iter: 76000\n",
      "loss: 50.800331534\n",
      "iter: 77000\n",
      "loss: 49.9533159668\n",
      "iter: 78000\n",
      "loss: 50.705504777\n",
      "iter: 79000\n",
      "loss: 50.0582776487\n",
      "iter: 80000\n",
      "loss: 50.3046559869\n",
      "iter: 81000\n",
      "loss: 49.9330965048\n",
      "iter: 82000\n",
      "loss: 50.0471812698\n",
      "iter: 83000\n",
      "loss: 50.1448162141\n",
      "iter: 84000\n",
      "loss: 50.2208953377\n",
      "iter: 85000\n",
      "loss: 50.6258852252\n",
      "iter: 86000\n",
      "loss: 50.8435569771\n",
      "iter: 87000\n",
      "loss: 50.6681248817\n",
      "iter: 88000\n",
      "loss: 50.7231780281\n",
      "iter: 89000\n",
      "loss: 50.1445941723\n",
      "iter: 90000\n",
      "loss: 50.5395843677\n",
      "iter: 91000\n",
      "loss: 50.296100756\n",
      "iter: 92000\n",
      "loss: 49.6950384036\n",
      "iter: 93000\n",
      "loss: 50.0052290572\n",
      "iter: 94000\n",
      "loss: 50.2277938839\n",
      "iter: 95000\n",
      "loss: 50.1196595972\n",
      "iter: 96000\n",
      "loss: 50.314370899\n",
      "iter: 97000\n",
      "loss: 49.9653522492\n",
      "iter: 98000\n",
      "loss: 50.5186747697\n",
      "iter: 99000\n",
      "loss: 50.6552380009\n",
      "iter: 100000\n",
      "loss: 49.9988154984\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    # check \"How to feed the loss function to see how this part works\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "        p = 0 # go from start of data                                                                                                                                                             \n",
    "    \n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print ('iter:',n) \n",
    "        print(\"loss:\",smooth_loss) # print progress\n",
    "        \n",
    "\n",
    "    # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    p += seq_length # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lorh thh  malt. \n",
      "ss when wes was to rept of to s ar mwagl. Lan Misan wowning urour to com \n",
      "tee of to the ondse the s aded abothed dine of thes be forne the pe the alpy?\" him din jo annisan hos the s cepp t heol title Gre dee thole the lees, na the yous acl-cearl sin as st wre led. Drior oll diens and tore he set cal. the weees, a ke st jee, ly rom saire. He kine sa blete not Setey as if hivs na t\n"
     ]
    }
   ],
   "source": [
    "sample(hprev, char_to_ix['A'], 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# code inspired by siraj raval's video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
